# egomotion

We, as team "**TheSSVL**" or "**EgoMotion-COMPASS**", took **2nd** place in both **Object State Change Classification** and **PNR temporal localization** tasks in Ego4d Challenge 2022  

Since we are currently working on our research project on Egocentric video understanding, we will make available our code used in Ego4d Challenge 2022 later in Oct.  

Besides, **our work on Egocentric video understanding** will be made publicly available by Nov 2022.  



TODO

- [ ] Post Techincal report on Arxiv  
- [ ] (Soon) Release model checkpoint and test codes that reproduces our results on Ego4d Challenge 2022  
- [ ] By late Oct, release codes of finetuning pretrained VideoMAE on Ego4d  
- [ ] By Nov, release codes of our latest work on egocentric video understading  

### Reference
[1] VideoMAE by Zhan, etc : [VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training](https://arxiv.org/abs/2203.12602)  
[2] VideoMAE by Kaiming, etc : [Masked Autoencoders As Spatiotemporal Learners](https://arxiv.org/abs/2205.09113)  
[3] Vanilla MAE: [Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377)  
[4] [Ego4D: Around the World in 3,000 Hours of Egocentric Video](https://arxiv.org/abs/2110.07058)  


### Contact
If you have any questions about our projects or implementation, please open an issue or contact via email:  
Jiachen Lei: jiachenlei@zju.edu.cn 

### Acknowledgements
We built our codes based on [VideoMAE](https://github.com/MCG-NJU/VideoMAE), [MAE-pytorch](https://github.com/pengzhiliang/MAE-pytorch). Thanks to all the contributors of these great repositories.
